---
title: "Gradient Descent Algorithms"
date: "2022-01-11"
readingTime: 2 mins
description: It's about to go down! üëá
tags: [ml, optimization, supervised]
img: "https://images.unsplash.com/photo-1482335976759-7a41d1fb8063?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxzZWFyY2h8NXx8ZGVzY2VudHxlbnwwfHwwfHw%3D&auto=format&fit=crop&w=900&q=60"
authors: ["Bijon Setyawan Raya"]
publish: true
series: true
---

<Series
  series="Gradient Descent Algorithms"
  posts={[
    {
      title: "Introduction",
      slug: "gradient-descent",
    },
    {
      title: "Linear Regression",
      slug: "linear-regression",
    },
    {
      title: "Mathematics of Gradient Descent",
      slug: "mathematics-of-gradient-descent",
    },
    {
      title: "Batch Gradient Descent",
      slug: "batch-gradient-descent",
    },
    {
      title: "Mini Batch Gradient Descent",
      slug: "mini-batch-gradient-descent",
    },
    {
      title: "Stochastic Gradient Descent",
      slug: "stochastic-gradient-descent",
    },
  ]}
  currentPost="Introduction"
/>

In this series, we are going to learn one of the well-known optimization algorithms, namely Gradient Descent algorithm.
However, we will go through the series gradually, so that the readers will not get overwhelmed with lots of details, especially the mathematical notations and formulas.

In the beginning, we are going to discuss what Linear Regression is and how to use it. Even though it's an easy concept and many resources can be found, it's still a good idea to refresh your memory before delving into Gradient Descent.
After that, we are going to learn the inner working of Gradient Descent. Then, we will start implementing the simplest form of Gradient Descent, which is Batch Gradient Descent.
By tweaking abit the formula and adding some complexities into the algorithm little by little, we can get all the way to developing Adam and many of its variations.

Here is the list of algorithms that we are going to cover:

1. Batch Gradient Descent ‚úÖ
2. Mini Batch Gradient Descent ‚úÖ
3. Stochastic Gradient Descent ‚úÖ
4. SGD with Momentum ‚ùå
5. SGD with Nesterov & Momentum ‚ùå
6. Many more ...

By the end of this series, we will be able to write optimization algorithms from scratch. Since this is my first attempt in writing scientific papers, please send me your feedback and suggestions via email.
Also, if you notice some mistakes or type in my posts, please do let me know.
