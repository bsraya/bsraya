---
title: RMSprop
description: Reducing the aggresive learning rate decay in Adagrad using the twin sibling of Adadelta.
date: 2024-05-04
tag: Optimization
published: true
type: Post
---

<SeriesTable
  title="Gradient Descent Algorithm"
  series={[
    {
      title: "Introduction to Gradient Descent Algorithm",
      current: false,
    },
    {
      title: "Mathematics of Gradient Descent",
      current: false,
    },
    {
      title: "Batch Gradient Descent",
      current: false,
    },
    {
      title: "Mini Batch Gradient Descent",
      current: false,
    },
    {
      title: "Stochastic Gradient Descent",
      current: false,
    },
    {
      title: "SGD with Momentum",
      current: false,
    },
    {
      title: "SGD with Nesterov",
      current: false,
    },
    {
      title: "Adagrad",
      current: false,
    },
    {
      title: "Adadelta",
      current: false,
    },
    {
      title: "RMSprop",
      current: true,
    },
    {
      title: "Adam",
      current: false,
    }
  ]}
/>

## Introduction

In the [Adadelta](/posts/adadelta) post, we have discussed the limitations of the Adagrad algorithm,
the rapid decay of the learning rate. Adadelta solves limitation by calculating the average squared of the decayed gradients over time.

RMSprop also solves the limitation of Adagrad just like Adadelta.
Unlike Adadelta The only difference is that RMSprop uses the learning rate value, and divides it with the root mean squared of the decayed gradients.
This algorithm was proposed independently by Geoffrey Hinton around the same time as Adagrad.
However, this algorithm was never published.

## Mathematics of RMSprop

RMSprop uses the following equation to update its parameters.

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
$$

where

- $\theta_t$ is the parameter at time $t$
- $\alpha$ is the learning rate
- $E[g^2]_t$ is the root mean squared of the decayed gradients up to time $t$

If we expand $E[g^2]_t$ we get

$$
E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1g_t^2
$$

## Implementation of RMSprop

First, we need to derive the gradient of the loss function $g_t$ with respect to the
intercept and the coefficient.

$$
\begin{aligned}
  \nabla_{\theta_0} J(\theta) &= \frac{\partial}{\partial \theta_0} J(\theta) \\
  &= \frac{\partial}{\partial \theta_0} \left( \frac{1}{2} (f(x_i) - y_i)^2 \right) \\
  &= f(x_i) - y_i
\end{aligned}
$$

$$
\begin{aligned}
  \nabla_{\theta_1} J(\theta) &= \frac{\partial}{\partial \theta_1} J(\theta)  \\
  &= \frac{\partial}{\partial \theta_1} \left( \frac{1}{2} (f(x_i) - y_i)^2 \right) \\
  &= (f(x_i) - y_i)x
\end{aligned}
$$

Notice that the gradient of the loss function with respect to the intercept is just the error of the prediction.

```python
random_index = np.random.randint(len(x))
prediction = predict(intercept, coefficient, x[random_index])
error = prediction - y[random_index]

new_intercept_gradient = error
new_coefficient_gradient = error * x[random_index]
```

Second, we need to calculate the running average of the squared gradients $E[g^2]_t$.
Note that $E[g^2]_{t-1}$ is the root mean squared of the gradients up to time $t-1$,
and $g_t$ is the gradient at time $t$.

$$
E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1g_t^2
$$

```python
mean_squared_intercept = (0.9 * np.mean(df['intercept_gradient'].values ** 2)) + (0.1 * new_intercept_gradient ** 2)
mean_squared_coefficient = (0.9 * np.mean(df['coefficient_gradient'].values ** 2)) + (0.1 * new_coefficient_gradient ** 2)
```

Lastly, we can update the parameters immediately using the calculation we have done above.

```python
intercept -= (learning_rate / np.sqrt(mean_squared_intercept + eps)) * new_intercept_gradient
coefficient -= (learning_rate / np.sqrt(mean_squared_coefficient + eps)) * new_coefficient_gradient
```

## Conclusion

![Pathways of Adagrad, Adadelta, and RMSprop along the 2D MSE contour](/assets/posts/rmsprop/pathways.png)

From the graph, we can see that Adagrad struggles to reach the bottom of the cost function in 150 iterations.
Unlike Adagrad, Adadelta and RMSprop can reach the bottom of the cost function easily.
However, the pathway of Adadelta is noisy compared to RMSprop, and RMSprop is more stable and direct compared to the other two algorithms.

## Code

```python
def rmsprop(x, y, df, epoch=150, learning_rate=0.01, eps=1e-8):
  intercept, coefficient = -0.5, -0.75

  random_index = np.random.randint(len(x))
  prediction = predict(intercept, coefficient, x[random_index])
  error = prediction - y[random_index]
  mse = (error ** 2) / 2
  df.loc[0] = [intercept, coefficient, error, error * x[random_index], mse]

  for epoch in range(1, epoch + 1):
    random_index = np.random.randint(len(x))
    prediction = predict(intercept, coefficient, x[random_index])
    error = prediction - y[random_index]

    new_intercept_gradient = error
    new_coefficient_gradient = error * x[random_index]

    mean_squared_intercept = (0.9 * np.mean(df['intercept_gradient'].values ** 2)) + (0.1 * new_intercept_gradient ** 2)
    mean_squared_coefficient = (0.9 * np.mean(df['coefficient_gradient'].values ** 2)) + (0.1 * new_coefficient_gradient ** 2)

    intercept -= (learning_rate / np.sqrt(mean_squared_intercept + eps)) * new_intercept_gradient
    coefficient -= (learning_rate / np.sqrt(mean_squared_coefficient + eps)) * new_coefficient_gradient

    mse = ((prediction - y[random_index]) ** 2) / 2
    df.loc[epoch] = [intercept, coefficient, new_intercept_gradient, new_coefficient_gradient, mse]

  return df
```

## References

1. Sebastian Ruder. "An overview of gradient descent optimization algorithms." [arXiv:1609.04747](https://arxiv.org/abs/1609.04747) (2016).
