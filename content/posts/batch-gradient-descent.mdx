---
title: Batch Gradient Descent
description: Linear Regression + Batch Gradient Descent in Python
date: 2022-02-16
tag: Optimization
type: Post
---

<SeriesTable
  title="Gradient Descent Algorithm"
  series={[
    {
      title: "Introduction to Gradient Descent Algorithm",
      current: false,
    },
    {
      title: "Mathematics of Gradient Descent",
      current: false,
    },
    {
      title: "Batch Gradient Descent",
      current: true,
    },
    {
      title: "Mini Batch Gradient Descent",
      current: false,
    },
    {
      title: "Stochastic Gradient Descent",
      current: false,
    },
    {
      title: "SGD with Momentum",
      current: false,
    },
    {
      title: "SGD with Nesterov",
      current: false,
    },
    {
      title: "Adagrad",
      current: false,
    },
    {
      title: "Adadelta",
      current: false,
    },
  ]}
/>

## Introduction

In the [Mathematics of Gradient Descent](post), we have discussed what Gradient Descent is, how it works, 
and how to derive the equations needed to update the parameters of the model.

In this post, we are going to write Batch Gradient Descent from scratch in Python.

## Setting Up The Dataset

Throughout this series, we are going to use the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) from UCI Machine Learning Repository imported from `scikit-learn`.
There are two features in the dataset that we are going to analyse, namely `sepal_length` and `petal_width` shown in the highlighted lines.

```python
from sklearn.datasets import load_iris

iris = datasets.load_iris()
features = iris.data
target = iris.target

sepal_length = np.array(features[:,0])
petal_width = np.array(features[:,3])

species_map = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
species_names = [species_map[i] for i in target]
```

## Setting Up A Baseline

Before we implement Batch Gradient Descent in Python, we need to set a baseline to compare against our own implementation.
So, we are going to train our dataset into the Linear Regression built-in function made by `scikit-learn`.

First, let's fit our dataset to `LinearRegression()` model that we imported from `sklearn.linear_model`.

```python
linreg = LinearRegression()

linreg.fit(
    X = sepal_length.reshape(-1,1),
    y = petal_width.reshape(-1,1)
)

print("Intercept: ",linreg.intercept_[0])
# Intercept: -3.200215
print("First coefficient:", linreg.coef_[0][0])
# First coeficient: 0.75291757
```

Once we have the intercept and the coefficient values, let's make a regression line to see if the line is close to most data points.

```python
sns.scatterplot(
    x = sepal_length,
    y = petal_width,
    hue = species_names
)

plt.plot(
    sepal_length,
    linreg.intercept_[0] +
    linreg.coef_[0][0] * features[:, 0],
    color='red'
)
```

![The iris dataset regression line with Scikit](/assets/posts/batch-gradient-descent/scikit-lr.png "The iris dataset regression line with Scikit")

Clearly, the line is indeed very close to the most data points and we want to see the MSE of this regression line.

```python
linreg_predictions = linreg.predict(sepal_length.reshape(-1,1))
linreg_mse = mean_squared_error(linreg_predictions, petal_width)
print(f"The MSE is {linreg_mse}")
# The MSE is 0.19101500769427357
```

From the result we got from `sklearn`, the best regression line is

$$
  y = -3.200215 + 0.75291757 \cdot x
$$

with MSE value around $0.191$. The equation above is going to be our base line for this experiment to determine how good our own Gradient Descent implementation.

## Implemetation of Batch Gradient Descent

Before we implement Batch Gradient Descent, we need to define the prediction function.

```python
def predict(intercept, coefficient, dataset):
  return intercept + coefficient * x
```

In order to update the intercept $\theta_0$ and the coefficient $\theta_1$ iteratively, we are going to use the following equations.

$$
\begin{aligned}
  \theta_0 &= \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta) \\
  \theta_1 &= \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta)
\end{aligned}
$$

where

$$
\begin{aligned}
\frac{\partial}{\partial \theta_0} J(\theta) &= \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2 \\ 
\frac{\partial}{\partial \theta_1} J(\theta) &= \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2 x \\ 
\end{aligned}
$$

First, let's determine the error term of the prediction.

```python
length = len(x)
error = prediction - y

t0_error = np.sum(error) / length
t1_error = np.sum(error * x) / length
```

Second, we update the intercept $\theta_0$ and the coefficient $\theta_1$.

```python
intercept = intercept - alpha * t0_error
coefficient = coefficient - alpha * t1_error
```

Finally, we can calculate the MSE of the regression line.

```python
mse = np.sum(error ** 2) / (2 * length)
```

## Conclusion

![BGD Loss Function Graph](/assets/posts/batch-gradient-descent/iterations.png "BGD Loss Function Graph")

![Regression line changes over time](/assets/posts/batch-gradient-descent/regression-lines.png "Regression line changes over time")

![Regression line animation](/assets/posts/batch-gradient-descent/bgd.gif "Regression line animation")


From the graph above, we can see that how the regression line changes from the time to time.
After $10,000$ iterations, the MSE value of our own Gradient Descent is $0.195$ which is quite close to our baseline, $0.191$.

![The pathway of the cost function over the 2D MSE contour](/assets/posts/batch-gradient-descent/contour.png "The movement of the intercept and the coefficient variabels on a contour map")

Here are some keypoints for Batch Gradient Descent:

1. Batch Gradient Descent only updates the parameters once after considering all the data points.
2. Since the parameters are updated once after considering all the data points, it takes longer time for the algorithm to converge.
3. Not only does Batch Gradient Descent takes a significant amount time to converge due to the large number of data points, but it also takes up a lot of computational resources.
4. Batch Gradient Descent is not the best algorithm for large datasets.
5. Since the gradients we get over time are pretty much the same, then it may stuck in a local minima. Noisy gradients would allow the algorithm to escape local minima.

## Code

```python
def bgd(x, y, epochs, df, alpha = 0.01):
  intercept, coefficient = 2.0, -7.5
  length = len(x)

  predictions = predict(intercept, coefficient, x)
  error = predictions - y
  mse = np.sum(error ** 2) / (2 * length)
  df.loc[0] = [intercept, coefficient, mse]

  for epoch in range(1, epochs):
    predictions = predict(intercept, coefficient, x)
    error = predictions - y
    t0_error = np.sum(error) / length
    t1_error = np.sum(error * x) / length
    intercept = intercept - alpha * t0_error
    coefficient = coefficient - alpha * t1_error
    mse = np.sum(error ** 2) / (2 * length)
    df.loc[epoch] = [intercept, coefficient, mse]
  return df
```

## References

1. M. Jack. _3D Gradient Descent in Python_. Source [https://jackmckew.dev/3d-gradient-descent-in-python.html](https://jackmckew.dev/3d-gradient-descent-in-python.html)
2. T. Arseny. _Gradient Descent From Scratch_. Source [https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc](https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc)
3. O. Artem. _Stochastic, Batch, and Mini-Batch Gradient Descent_. Source [https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5](https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5)
4. P. Sushant. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)
5. Geeksforgeeks. _Difference between Batch Gradient Descent and Stochastic Gradient Descent_. Source [https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/)
6. Sweta. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)
7. Geeksforgeeks. _ML | Mini-Batch Gradient Descent with Python_. Source [https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/)
