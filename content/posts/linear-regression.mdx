---
title: Linear Regression
description: A gentle introduction to Linear Regression
date: 2022-02-01
tag: Machine Learning
type: Post
---

<SeriesTable
  title="Regression Algorithms"
  series={[
    {
      title: "Linear Regression",
      current: true,
    },
    {
      title: "Logistic Regression",
      current: false,
    },
    {
      title: "Lasso Regression",
      current: false,
    },
    {
      title: "Ridge Regression",
      current: false,
    },
    {
      title: "Elastic Net Regression",
      current: false,
    },
  ]}
/>

## What is Linear Regression?

Linear Regression is one of many supervised machine learning algorithms, and it is mosly used to predict the value of a continuous variable, as well as to do forecasting.
In other words, it can be used:
1. to see if one variable can be used to predict another variable.
2. to see if one variable is correlated or dependent with another variable.

However, Linear Regression also comes with some limitations, such as:
1. It assumes that the relationship between the independent variable and the dependent variable is linear. However, in reality, the relationship between the independent variable and the dependent variable is not always linear.
2. It assumes that the independent variables are not correlated with each other. However, in reality, the independent variables are not always independent.
3. It's sensitive to outliers. Meaning that the presence of outliers can affect the regression line.

<LineGraph
  data={{
    labels: [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
    datasets: [
      {
        label: "y = b + ax",
        data: [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
        borderColor: "blue",
      },
    ],
  }}
  options={{
    plugins: {
      legend: {
        display: true,
        position: "bottom",
      },
    },
  }}
/>

From the graph above, we can see that the relationship between $x$ and $y$ is linear since the blue line starts from the bottom left to the top right.
That line is called a regression line, and it can be expressed using the following equation.

$$
  y = \beta_0 + \beta_1 \times x
$$

where $\beta_0$ is the intercept and $\beta_1$ is the first coefficient. 
In high school or college, we are used to seeing the equation above in the following form to calculate the distance between one point to another.

$$
  y = b + ax
$$

In this post, we are going to use two features in the Iris dataset from `sklearn-learn`, petal width and sepal length. Plotting it will give us the following visualization.

![Iris Dataset Scatter Plot](/assets/posts/linear-regression/iris_scatter.png "Iris Dataset Scatter Plot")

You should know that the intercept or $\beta_0$ is the starting point of the regression line.
Whether the line is going up or down depends on the $\beta_1$ and the data.
If $\beta_0 = 0$, it means that our regression line will start from $0$.

Expressing the equation like we did above is quite cryptic for people who don't have strong mathematical background.
Since we are using the Iris Dataset, we can translate the equation into a more readable form.

$$
  \text{petal width} = \beta_0 + \beta_1 \times \text{sepal length}
$$

From the translation above can tell us the relationship between those two variables.
Now you will be wondering their correlations whether `sepal_length` and `petal_width` are correlated or inversely correlated.
First, let's translate what the two graphs below are trying to tell us.

<div style={{ display: "flex", justifyContent: "space-between", width: "50%" }}>
  <LineGraph
    data={{
      labels: [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
      datasets: [
        {
          label: "Positively Correlated Regression Line",
          data: [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
          borderColor: "green",
        },
      ],
    }}
    options={{
      plugins: {
        legend: {
          display: true,
          position: "bottom",
        },
      },
    }}
  />
  <LineGraph
    data={{
      labels: [3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0],
      datasets: [
        {
          label: "Negatively Correlated Regression Line",
          data: [3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0],
          borderColor: "red",
        },
      ],
    }}
    options={{
      plugins: {
        legend: {
          display: true,
          position: "bottom",
        },
      },
    }}
  />
</div>

`sepal_length` and `petal_width` are said to be correlated when `sepal_length` increases, the `petal_length` also increases.
Conversely, `sepal_length` and `petal_width` are said to be inversely correlated when `sepal_length` increases, but the `petal_width` decreases.

With a regression line, it can help us to predict the $y$ value given a single $x$ value.
However, most predictions made by the regression line are not always accurate
since its ability to predict depends heavily on $\beta_0$ and $\beta_1$.
If the values $\beta_0$ and $\beta_1$ are not tweaked correctly, the regression line will sit right far from most data points.

## Examples

In this section, there will be two examples of regression lines. One with the regression line is far from most data points, and the other one is close to most data points.

![Most data points are far from the regression line](/assets/posts/linear-regression/far.png "Most data points are far from the regression line")

When the regression line, which is indicated by the green line, sees $x = 0$ then it predicts $y = -3$.
In reality, $y$ should be $0.5$ when $x = 0$. Meaning that the predicted value is far from the actual value.

There are many ways to measure the quality of regression lines, such as:
1. R Squared
2. Mean Squared Error (MSE)
3. Root Mean Squared Error (RMSE)
4. Mean Absolute Error (MAE)

However, we are going to use Mean Squared Error (MSE) this time.

$$
  MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
$$

Let's calculate the MSE for the graph above with the data points above.

$$
  \begin{aligned}
    MSE &= \frac{47.59}{5} \\
    &= 9.518
  \end{aligned}
$$

where $n$ is the number of data points, $\hat{y}_i$ is the predicted value, and $y_i$ is the actual value.

Let's see another example where the data points are close to the regression line.

![Most data points are close from the regression line](/assets/posts/linear-regression/close.png "Most data points are close from the regression line")

Let's calculate the MSE for this example to see if the MSE is small when the regression line is close to most data points.

$$
  \begin{aligned}
    MSE &= \frac{1.74}{5} \\
    &= 0.348
  \end{aligned}
$$

From these two examples, we can see that when the regression line is close to most data points, the MSE is small.
Conversonly, when the regression line is far from most data points, the MSE is large.

Is having a small MSE enough to say that the regression line is good? Futher investigation is needed to answer this question.
However, I am not gonna cover it in this post.

## Coding Implementation

First prepare the dataset. We are going to use the Iris dataset from `sklearn-learn` with two features, `petal_width` and `sepal_length`.

```python
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import seaborn as sns

iris = load_iris()
sepal_length = iris.data[:, 0]
petal_width = iris.data[:, 3]
target = iris.target

species_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
species_name = [species_dict[i] for i in target]
```

Note that, the regression line is calculated using the following equation.

$$
  \hat{y} = \beta_0 + \beta_1 \times x
$$

```python
b, x = 0, 0

regression_line = [
  b + sepal_length[i] * x for i in range(len(sepal_length))
]
```

where $\beta_0$ is the intercept and $\beta_1$ is the first coefficient.
In this case, we are going to replace $\beta_0$ with `b` and $\beta_1$ with `x`.
Without any assumption, we are going to start with `b=0` and `x=0`.

```python
sns.scatterplot(x = sepal_length, y = petal_width, hue=species_name)
plt.plot(sepal_length, y, c='r')
plt.title('Iris Dataset: Sepal Length vs Petal Width')
plt.xlabel('Sepal Length')
plt.ylabel('Petal Width')
plt.legend()
plt.show()
```

![Regression line with b=0 and x=0](/assets/posts/linear-regression/initial-guess.png "Regression line with b=0 and x=0")

We could just keep on guessing the value of `b` and `x` until we get the best regression line.
However, we aren't gonna keep on guessing, and it's better to use a more systematic approach such as Gradient Descent algorithm.
Thus, we gotta customize the code a little so that we can automate the process of estimating `b` and `x`.

If you are want to know more about Gradient Descent algorithm, you can read the Gradient Descent series [here](/posts/introduction-to-gradient-descent).
The series covers the intuition behind Gradient Descent, the math behind it, and the implementation in Python.

```python
def predict(b, x, data):
  return [b + data[i] * x for i in range(len(data))]

def gradient_descent(x, y, epochs, alpha = 0.01):
  intercept, coefficient = 0, 0

  for _ in range(1, epochs):
    predictions = predict(intercept, coefficient, x)
    intercept = intercept - alpha * np.sum(predictions - y) / len(x)
    coefficient = coefficient - alpha * np.sum((predictions - y) * x) / len(x)

  return intercept, coefficient

b, x = gradient_descent(sepal_length, petal_width, 10000)
# b = -2.717366489030271, x = 0.6718570469763597
```

After running the Gradient Descent algorithm for $10,000$ times, we get the following regression line.

![Regression line with b=-2.71 and x=0.67](/assets/posts/linear-regression/corrected-regression-line.png "Regression line with b=-2.71 and x=0.67")

You can find the full code in [this repository](https://github.com/bsraya/machine-learning-algorithms/blob/main/linear-regression.ipynb)

## Conclusion

1. Linear Regression is a supervised learning algorithm that is used to predict the value of a continuous variable.
2. The equation of the regression line is $y = \beta_0 + \beta_1 \times x$.
3. The regression line is said to be good when it is close to most data points.
4. The quality of the regression line can be measured using evaluation metrics, one of them is Mean Squared Error (MSE).
5. The smaller the MSE, the closer the regression line is to most data points. Conversely, the larger the MSE, the farther the regression line is to most data points.
6. `b` and `x` can be estimated using Gradient Descent algorithm.

## References

1. Wikipedia. _Linear Regression_. [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)
2. Wikipedia. _Gradient Descent_. [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)