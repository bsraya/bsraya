---
title: Logistic Regression
description: Redoing Logistic Regression from scratch in Python
date: 2023-12-24
tag: Machine Learning
type: Post
---

<SeriesTable
  title="Regression Algorithms"
  series={[
    {
      title: "Linear Regression",
      current: false,
    },
    {
      title: "Logistic Regression",
      current: true,
    },
    {
      title: "Lasso Regression",
      current: false,
    },
    {
      title: "Ridge Regression",
      current: false,
    },
    {
      title: "Elastic Net Regression",
      current: false,
    },
  ]}
/>

Well as the description suggests, we are going to redo Logistic Regression from scratch.
Most importantly, we are going to go through the mathematical equations behind it and implement it as quickly as possible in Python.
We will be using the same dataset as we used in the previous post.

## What is Logistic Regression?

Logistic Regression is one of many supervised machine learning algorithms, just like Linear Regression.
Instead of predicting a continuous value, it predicts the probability of an event occurring.
So, this algorithms is mostly used for binary classification problems. Here are the use cases of Logistic Regression:
1. Predict if an email is spam or not.
2. Predict if a credit card transaction is fraudulent or not.
3. Predict if a customer will churn or not.
4. Predict if a patient has cancer or not.

However, it also has the same limitations as Linear Regression, such as:
1. It is a linear model, so it cannot capture complex non-linear relationships. In other words, it assumes that the data is linearly separable.
2. It assumes that the data is independent of each other. In other words, it assumes that the data is not correlated with each other.
3. It is sensitive to outliers.

## Mathematical Equations

First, Logistic Regression still uses Linear Equation used in Linear Regression, and it's expressed as:
$$
\hat{y} = \beta_0 + \beta_1x
$$

Second, we are going to need Sigmoid function. We are going to need this function to do prediction by converting the output of the linear equation into a probability value between 0 and 1.

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

where $z = \beta_0 + \beta_1 \times x$. Then the sigmoid function can be rewritten as:

$$
\sigma(\beta_0 + \beta_1x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
$$

After acquiring the probability value from the Sigmoid function, we can use a threshold value to classify into one or another.
To generalize, we can use the following equation to predict the probability of an event occurring:

$$
\begin{aligned}
P(y | x) &= \frac{1}{1 + e^{-x}} \\
&= \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
\end{aligned}
$$

If $P(y | x) \geq 0.5$, then the data is classified as $1$, otherwise it is classified as $0$.

## Implementation

First things first, we need to import the necessary libraries.

```python
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
```

Say that we only have a feature $x$, the sepal length, and we want to determine the instance is a Virginica or not.
Let's prepare the data.

```python
iris = load_iris()
sepal_length = iris.data[:, 0]
target = iris.target

is_virginica_dict = {0: 0, 1: 0, 2: 1}
is_virginica = np.array([is_virginica_dict[i] for i in target])

species_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
species_name = np.array([species_dict[i] for i in target])
```

![Data points are either sitting at 0 or 1](/assets/posts/logistic-regression/data.png)

You might be wondering why we have `is_virginica_dict`.
I need that variable to separate the data so that some data sit at the bottom of the plot and some sit at the top of the plot.
Let's add a Linear Regression Line to the plot. 

![A graph with a Linear Regression line](/assets/posts/logistic-regression/non-linear.png)

In order to draw that line, I set the intercept as $-1.2$ and $0.28$ as the coefficient.

It's clear that the straight line can't help us capture the overall trend of the data. 
Thus, we need to use Sigmoid function to bend the line, so that the line would look like this:

![A graph with a Logistic Regression line](/assets/posts/logistic-regression/logreg.png)

Most importantly, we need to include these following functions to train and update the intercept and the coefficient values.
From here, we are going start from $-1.2$ and $0.28$ as the initial guess.

```python
def accuracy(y_pred, y):
  return np.sum(y_pred == y) / len(y)

def sigmoid(x):
  if x >= 0:
    z = np.exp(-x)
    return 1 / (1 + z)
  else:
    z = np.exp(x)
    return z / (1 + z)
    
def linear_function(intercept, coefficient, x):
  return intercept + coefficient * x

def threshold(x):
  return np.where(x > 0.5, 1, 0)

def gradient_descent(x, y, epochs, alpha = 0.01):
  intercept, coefficient = -1.2, 0.28

  for _ in range(epochs):
    y_pred = np.array(
      [
        sigmoid(
          linear_function(intercept, coefficient, i)
        ) for i in x
      ]
    )
    intercept = intercept - alpha * np.sum(y_pred - y) / len(y)
    coefficient = coefficient - alpha * np.sum((y_pred - y) * x) / len(y)

  return intercept, coefficient
```

Let's train our model for $100,000$ epochs.

```python
intercept, coefficient = gradient_descent(sepal_length, is_virginica, 100000)
predicted_value = np.array([sigmoid(linear_function(intercept, coefficient, i)) for i in sepal_length])
corrected_prediction = threshold(predicted_value)

print('accuracy: ', accuracy(corrected_prediction, is_virginica))
print('intercept: ', intercept)
print('coefficient: ', coefficient)
# accuracy:  0.8
# intercept:  -13.004847396222699
# coefficient:  2.0547824850027654
```

Not bad, the accuracy is $80\%$, with $-13.00$ as the intercept and $2.05$ as the coefficient.

You can find the full code in [this repository](https://github.com/bsraya/machine-learning-algorithms/blob/main/logistic-regression.ipynb)

## Conclusion

1. Logistic Regression is a supervised machine learning algorithm that is used for binary classification problems.
3. It uses the Sigmoid function to calculate the probability of an event occurring.
4. It uses a threshold value to roundup the probability value to either $0$ or $1$. If $\delta(x) > 0.5$, then the data is classified as $1$, otherwise it is classified as $0$.

## References

1. Vieira, Tim. _Exp-Normalize Trick_. [https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/](https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)