---
title: Adam
description: The best version of all adaptive learning rate optimization algorithms
date: 2024-05-05
tag: Optimization
published: true
type: Post
---

<SeriesTable
  title="Gradient Descent Algorithm"
  series={[
    {
      title: "Introduction to Gradient Descent Algorithm",
      current: false,
    },
    {
      title: "Mathematics of Gradient Descent",
      current: false,
    },
    {
      title: "Batch Gradient Descent",
      current: false,
    },
    {
      title: "Mini Batch Gradient Descent",
      current: false,
    },
    {
      title: "Stochastic Gradient Descent",
      current: false,
    },
    {
      title: "SGD with Momentum",
      current: false,
    },
    {
      title: "SGD with Nesterov",
      current: false,
    },
    {
      title: "Adagrad",
      current: false,
    },
    {
      title: "Adadelta",
      current: false,
    },
    {
      title: "RMSprop",
      current: false,
    },
    {
      title: "Adam",
      current: true,
    }
  ]}
/>

## Introduction

## Mathematics of Adam

## Implementation of Adam

## Conclusion

## Code

## References

1. Sebastian Ruder. "An overview of gradient descent optimization algorithms." [arXiv:1609.04747](https://arxiv.org/abs/1609.04747) (2016).
2. Diederik P. Kingma, Jimmy Ba. "Adam: A Method for Stochastic Optimization." [arXiv:1412.6980](https://arxiv.org/abs/1412.6980) (2014).
