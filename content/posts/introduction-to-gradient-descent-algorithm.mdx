---
title: Introduction to Gradient Descent Algorithm
description: It's about to go down! ðŸ‘‡
date: 2022-01-11
tag: Machine Learning
type: Post
---

<SeriesTable
  title="Gradient Descent Algorithm"
  series={[
    {
      title: "Introduction to Gradient Descent",
      current: true,
    },
    {
      title: "Mathematics of Gradient Descent",
      current: false,
    },
    {
      title: "Batch Gradient Descent",
      current: false,
    },
    {
      title: "Mini Batch Gradient Descent",
      current: false,
    },
    {
      title: "Stochastic Gradient Descent",
      current: false,
    },
  ]}
/>

In this series, we are going to learn one of the well-known optimization algorithms, namely Gradient Descent algorithm.
However, we will go through the series gradually, so that the readers will not get overwhelmed with lots of details, especially the mathematical notations and formulas.

First things first, we will start implementing the simplest form of Gradient Descent, which is Batch Gradient Descent.
By tweaking abit the formula and adding some complexities into the algorithm little by little, we can get all the way to developing Adam and many of its variations.

Here is the list of algorithms that we are going to cover:

1. Batch Gradient Descent âœ…
2. Mini Batch Gradient Descent âœ…
3. Stochastic Gradient Descent âœ…
4. SGD with Momentum (Coming Soon)
5. SGD with Nesterov & Momentum (Coming Soon)
6. Adam (Coming Soon)
7. AdaGrad (Coming Soon)
7. Many more ...

By the end of this series, we will be able to write optimization algorithms from scratch in Python.