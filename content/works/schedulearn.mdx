---
title: Schedulearn
description: A lightweight scheduling system for deep learning models
date: 2022-02-01
image: /assets/works/schedulearn/thumbnail.png
tag: System Architecture
type: Work
---

## Introduction

Schedulearn, stands for **Schedule** + **Learn**, is a lightweight scheduling system for deep learning models.
It is designed to be a simple and easy-to-use system for scheduling deep learning models so that users can focus on developing their models without constantly worrying about resources. 

## Tech Stacks

### Backend
1. FastAPI
2. SQLModel
3. Pydantic
4. SQLite
5. Docker
6. Horovod

![Schedulearn consists of 3 components: UI, API, and the server](/assets/works/schedulearn/schedulearn-architecture.png)

### Overall Architecture

In the figure above, you can see that there are three main components in the system: the **API**, the **servers**, the **user interface**. 
Those API endpoints are responsible for handling users requests from the user interface, such as creating, updating, and deleting models.
The database is responsible for storing the models' metadata.
The scheduler, which is built in the same place as the API, is the core of the system and is responsible for scheduling the models depending on the resources available in the cluster.
Meaning, the system will tell where each model should be trained at and how much resources each model should use.

There are three scheduling algorithms exist within the system:

1. First-In-First-Out (FIFO)
2. Round-Robin (RR)
3. Elastic First-In-First-Out (EFIFO)

![Job Submission Procedures](/assets/works/schedulearn/submission-procedure.png "Job Submission Procedures")

When a job is submitted, that particular job will undergo several steps before its results are being sent back to the user.

1. The job is sent to the API
2. The API will save the job's metadata in the database, schedule the job, and send the job to its corresponding server. 
3. The server will then pull the job from the API and start training the model.
4. The server sends the result back to the API and save the result in the database
5. The API sends back the result to the user

## Experiment Setup

The system consists of three servers. The following of the specifications of the entire system:

1. 3 x 4 Nvidia GTX 1080 Ti graphical processing units, each equipped with 11GB Video Random Access Memory (VRAM)
2. Intel Xeon E5-2678 v3 with 48 cores running at 2.50GHz
3. 128 GB Random Access Memory (RAM)
4. 10G PCIe NIC network card

## Challenges

Due security measures imposed by the lab that I worked for, I am unable to train models with multiple GPUs from different servers. 
However, assigning more GPUs does not translate to 1 to 1 performance increase. Meaning that training a model with 4 GPUs does not mean that the model will be trained 4 times faster, assuming that 1 GPU is 1x speed. 

![Scability across different Deep Learning libraries](/assets/works/schedulearn/scalability.png "Scability across different Deep Learning libraries")

To overcome this issue, each job will only be assigned to GPUs from the same server. 
Not only that is the only work around, but it is one of the ways to reduce communication costs.
Having multiple GPUs requires the data will have to travel more, which already imposes a significant overhead.
The further the data travels, the more overhead it will impose. Therefore, it's better to asssign multiple GPUs from the same server to a certrain job.

## Observations

![TensorFlow Makespan and Turnaround Time](/assets/works/schedulearn/tf.png "TensorFlow Makespan and Turnaround Time")

![PyTorch Makespan and Turnaround Time](/assets/works/schedulearn/py.png "PyTorch Makespan and Turnaround Time")

![Makespan and Turnaround Time of multiple scheduling algorithms](/assets/works/schedulearn/algo-comparison.png "Makespan and Turnaround Time of multiple scheduling algorithms")

## Conclusion

## Future Works